best performing model results

Data (x) dimension: (5818, 2590)
Label (y) dimension: (5818, 2)
y_test shape is (1163, 2)
valdata[1].shape (1163, 2)
Number of positive and negative examples in trainging and test data 
[ 3489.  1166.]
[ 875.  288.]
model fitting summary: CNN with multiple filters of different window sizes
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 2590)          0                                            
____________________________________________________________________________________________________
embedding_1 (Embedding)          (None, 2590, 100)     2462400     input_1[0][0]                    
____________________________________________________________________________________________________
conv1d_1 (Conv1D)                (None, 2588, 128)     38528       embedding_1[0][0]                
____________________________________________________________________________________________________
conv1d_2 (Conv1D)                (None, 2586, 128)     64128       embedding_1[0][0]                
____________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)   (None, 517, 128)      0           conv1d_1[0][0]                   
____________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)   (None, 517, 128)      0           conv1d_2[0][0]                   
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 1034, 128)     0           max_pooling1d_1[0][0]            
                                                                   max_pooling1d_2[0][0]            
____________________________________________________________________________________________________
conv1d_3 (Conv1D)                (None, 1030, 128)     82048       merge_1[0][0]                    
____________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)   (None, 206, 128)      0           conv1d_3[0][0]                   
____________________________________________________________________________________________________
conv1d_4 (Conv1D)                (None, 202, 128)      82048       max_pooling1d_3[0][0]            
____________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)   (None, 6, 128)        0           conv1d_4[0][0]                   
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 768)           0           max_pooling1d_4[0][0]            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           98432       flatten_1[0][0]                  
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 2)             258         dense_1[0][0]                    
====================================================================================================
Total params: 2,827,842
Trainable params: 2,827,842
Non-trainable params: 0
____________________________________________________________________________________________________
Train on 4655 samples, validate on 1163 samples
Epoch 1/20
4655/4655 [==============================] - 439s - loss: 0.5307 - categorical_accuracy: 0.7325 - val_loss: 0.5229 - val_categorical_accuracy: 0.7524 - val_auc: 0.7611
Epoch 2/20
4655/4655 [==============================] - 455s - loss: 0.4621 - categorical_accuracy: 0.7600 - val_loss: 0.4559 - val_categorical_accuracy: 0.7721 - val_auc: 0.8140
Epoch 3/20
4655/4655 [==============================] - 455s - loss: 0.3575 - categorical_accuracy: 0.8344 - val_loss: 0.3225 - val_categorical_accuracy: 0.8435 - val_auc: 0.9102
Epoch 4/20
4655/4655 [==============================] - 453s - loss: 0.2541 - categorical_accuracy: 0.8898 - val_loss: 0.2646 - val_categorical_accuracy: 0.8831 - val_auc: 0.9419
Epoch 5/20
4655/4655 [==============================] - 452s - loss: 0.1917 - categorical_accuracy: 0.9190 - val_loss: 0.3132 - val_categorical_accuracy: 0.8831 - val_auc: 0.9472
Epoch 6/20
4655/4655 [==============================] - 454s - loss: 0.1515 - categorical_accuracy: 0.9392 - val_loss: 0.2683 - val_categorical_accuracy: 0.8839 - val_auc: 0.9470
Epoch 7/20
4655/4655 [==============================] - 443s - loss: 0.1327 - categorical_accuracy: 0.9446 - val_loss: 0.2715 - val_categorical_accuracy: 0.8934 - val_auc: 0.9555
Epoch 8/20
4655/4655 [==============================] - 442s - loss: 0.1115 - categorical_accuracy: 0.9560 - val_loss: 0.3206 - val_categorical_accuracy: 0.8960 - val_auc: 0.9481
Epoch 9/20
4655/4655 [==============================] - 436s - loss: 0.0957 - categorical_accuracy: 0.9603 - val_loss: 0.2990 - val_categorical_accuracy: 0.8934 - val_auc: 0.9562
Epoch 10/20
4655/4655 [==============================] - 439s - loss: 0.0884 - categorical_accuracy: 0.9648 - val_loss: 0.2605 - val_categorical_accuracy: 0.8899 - val_auc: 0.9562
Epoch 11/20
4655/4655 [==============================] - 453s - loss: 0.0805 - categorical_accuracy: 0.9682 - val_loss: 0.2969 - val_categorical_accuracy: 0.8934 - val_auc: 0.9549
Epoch 12/20
4655/4655 [==============================] - 452s - loss: 0.0737 - categorical_accuracy: 0.9673 - val_loss: 0.3582 - val_categorical_accuracy: 0.8985 - val_auc: 0.9558
Epoch 13/20
4655/4655 [==============================] - 459s - loss: 0.0675 - categorical_accuracy: 0.9731 - val_loss: 0.3762 - val_categorical_accuracy: 0.8925 - val_auc: 0.9511
Epoch 14/20
4655/4655 [==============================] - 454s - loss: 0.0631 - categorical_accuracy: 0.9710 - val_loss: 0.3911 - val_categorical_accuracy: 0.9003 - val_auc: 0.9551
Epoch 15/20
4655/4655 [==============================] - 451s - loss: 0.0611 - categorical_accuracy: 0.9731 - val_loss: 0.2958 - val_categorical_accuracy: 0.8960 - val_auc: 0.9588


===================================================================================================================

Epoch 16/20
4655/4655 [==============================] - 454s - loss: 0.0588 - categorical_accuracy: 0.9725 - val_loss: 0.3061 - val_categorical_accuracy: 0.9003 - val_auc: 0.9573
Epoch 17/20
4655/4655 [==============================] - 448s - loss: 0.0517 - categorical_accuracy: 0.9755 - val_loss: 0.3972 - val_categorical_accuracy: 0.8994 - val_auc: 0.9523
Epoch 18/20
4655/4655 [==============================] - 450s - loss: 0.0533 - categorical_accuracy: 0.9757 - val_loss: 0.4349 - val_categorical_accuracy: 0.8977 - val_auc: 0.9533
Epoch 19/20
4655/4655 [==============================] - 451s - loss: 0.0494 - categorical_accuracy: 0.9751 - val_loss: 0.5078 - val_categorical_accuracy: 0.9011 - val_auc: 0.9508
Epoch 20/20
4655/4655 [==============================] - 457s - loss: 0.0494 - categorical_accuracy: 0.9766 - val_loss: 0.5245 - val_categorical_accuracy: 0.9054 - val_auc: 0.9482

AUC

[0.53074556001403017, 0.46210246890879342, 0.35747284717001543, 0.25412978647410805, 0.19167351028716909, 0.15146144561301497, 0.13273514726076166, 0.11152209442040846, 0.095743902677432033, 0.088446303721913463, 0.08048339240597091, 0.073665472421350978, 0.067530894577183087, 0.063139402579664231, 0.061076441676096806, 0.058788102304224807, 0.051662618895766559, 0.053348885098858372, 0.049423578814288278, 0.049447879889453775]
[0.761063492063492, 0.81401984126984117, 0.91017460317460319, 0.94191666666666662, 0.94719444444444445, 0.94696825396825401, 0.95548015873015868, 0.94812400793650808, 0.95622718253968264, 0.95620039682539693, 0.95486309523809521, 0.95575595238095234, 0.95110515873015877, 0.95509722222222226, 0.95880555555555569, 0.95728174603174598, 0.95229861111111114, 0.95334920634920639, 0.95082738095238106, 0.94824603174603173]


real	151m9.215s
user	482m26.648s
sys	32m15.104s

